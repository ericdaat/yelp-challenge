{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Sentiment Analysis over Yelp reviews\n",
    "\n",
    "This notebook shows how to build a text classifier that can predict whether a review is negative or positive. We used Yelp dataset, made public thanks to the Yelp-challenge. I am only doing this for fun and practice, please don't hesitate to correct me if you see any mistake or if you have any question. The notebook will be improved overtime when I implement newer algorithms and corrections. \n",
    "\n",
    "The Yelp reviews consist in a list of json documents that contain the following information:\n",
    "    \n",
    "- business_id\n",
    "- date\n",
    "- review_id\n",
    "- stars\n",
    "- text\n",
    "- type\n",
    "- user_id\n",
    "- votes\n",
    "    \n",
    "For classifying our reviews, we are going to use a Naive Bayes approach at fist, then use fancier model and see how it improves. Classification problem is a supervised learning task, which means we deal with labelled data. Here, every review has a rating, which consist of stars from 1 to 5. For our classifier, we are going to use two classes: `pos` and `neg` for positive versus negative reviews. As an arbitrary choice, we will say that a positve review is rated above 3 stars. \n",
    "\n",
    "I am programming in Python, along with libraries like `scikit-learn` or `gensim`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Building a Corpus Class\n",
    "\n",
    "To have an easier access to our data, we are going to build a Class called `JsonCorpus` and its parent Class called `Corpus`. \n",
    "\n",
    "`Corpus` will have two basics methods:\n",
    "\n",
    "- `__init__(self, path)` for registering dataset path as an attribute\n",
    "- `__iter__(self)` so we can read the full dataset review by review. Since the dataset is pretty big, the whole thing won't fit in RAM. Thanks to this approach, we can train our model on the whole corpus one review at a time. \n",
    "\n",
    "`JsonCorpus` will add a few methods to our `Corpus` class:\n",
    "\n",
    "- `__parse_json(self, line)` which is private method, that parse a json string representation to a json object\n",
    "- `__stars_to_sentiment(self, stars, pos_threshold)` is a private method that converts stars rating to `pos` or `neg` depending on `pos_threshold`.\n",
    "- `head(self, n, return_type)` that returns the first `n` reviews in a specified format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        \n",
    "        \n",
    "    def __iter__(self):\n",
    "        for line in open(self.path):\n",
    "            yield line\n",
    "\n",
    "\n",
    "class JsonCorpus(Corpus):\n",
    "    def __init__(self, path):\n",
    "        super(JsonCorpus, self).__init__(path)\n",
    "            \n",
    "            \n",
    "    def __parse_json(self, line):\n",
    "        return json.loads(line)\n",
    "    \n",
    "    \n",
    "    def __stars_to_sentiment(self, star, pos_threshold=3):\n",
    "        return 'pos' if star > pos_threshold else 'neg'\n",
    "            \n",
    "        \n",
    "    def head(self, n=1, return_type='json'):\n",
    "        with open(self.path) as file:\n",
    "            json = [self.__parse_json(next(file).strip()) for x in xrange(n)]\n",
    "            \n",
    "            if return_type is 'json':\n",
    "                # returns a list of plain json documents\n",
    "                return json\n",
    "            elif return_type is 'text_rating':\n",
    "                # returns a list of [text, stars] documents\n",
    "                return [[j['text'], j['stars']] for j in json]\n",
    "            elif return_type is 'text_sentiment':\n",
    "                # returns a list of [text, sentiment] documents\n",
    "                return [[j['text'], self.__stars_to_sentiment(j['stars'])] for j in json]\n",
    "            else:\n",
    "                raise NameError('invalid return_type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "corpus = JsonCorpus('../dataset/yelp_academic_dataset_review.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'business_id': u'5UmKMjUEUNdYWqANhGckJw',\n",
       "  u'date': u'2012-08-01',\n",
       "  u'review_id': u'Ya85v4eqdd6k9Od8HbQjyA',\n",
       "  u'stars': 4,\n",
       "  u'text': u'Mr Hoagie is an institution. Walking in, it does seem like a throwback to 30 years ago, old fashioned menu board, booths out of the 70s, and a large selection of food. Their speciality is the Italian Hoagie, and it is voted the best in the area year after year. I usually order the burger, while the patties are obviously cooked from frozen, all of the other ingredients are very fresh. Overall, its a good alternative to Subway, which is down the road.',\n",
       "  u'type': u'review',\n",
       "  u'user_id': u'PUFPaY9KxDAcGqfsorJp3Q',\n",
       "  u'votes': {u'cool': 0, u'funny': 0, u'useful': 0}}]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Mr Hoagie is an institution. Walking in, it does seem like a throwback to 30 years ago, old fashioned menu board, booths out of the 70s, and a large selection of food. Their speciality is the Italian Hoagie, and it is voted the best in the area year after year. I usually order the burger, while the patties are obviously cooked from frozen, all of the other ingredients are very fresh. Overall, its a good alternative to Subway, which is down the road.',\n",
       "  'pos']]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head(1, 'text_sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "## NLP with scikit-learn\n",
    "\n",
    "Now that our corpus is ready, let's dive into Natural Language Processing! We are first going to use `scikit-learn` library, as it provides a lot of algorithms, and is pretty easy to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Introduction to classification, a playground example\n",
    "\n",
    "With this playground example, we are goin to build a classifier prototype to understand how `scikit-learn` works, and what classification is about.\n",
    "\n",
    "The first idea is to convert raw words into vectors, so that the algorithm can understand them and classify sentences. We use what is called a bag of words approach: we gather all the words occuring in our corpus as a vocabulary base, giving us a `N` dimensional space, where `N` is how many unique words we have in the vocabulary. We can then map our sentences as vectors in our `N` dimensional space according to the words they contains. Things get clearer with a simple example:\n",
    "\n",
    "Let's say our corpus consists of two sentences \"I feel good\" and \"I feel bad\". Our vocabulary is `[\"I\", \"feel\", \"good\", \"bad\"]`. This is a 4 dimensional space in which our two sentences can be expressed as vectors of components `[1, 1, 1, 0]` and `[1, 1, 0, 1]`. Basically, we can say we have \"vectorized\" our textual data by expressing our sentences as vectors in a high dimensional space. High dimensional because vocabulary size gets very big, hence the size of our vector space. Our two vectors form a term-document matrix, because every document (review) is mapped to terms in our vocabulary.\n",
    "\n",
    "Then, our algorithm is going to learn how to classify based on the training data we feed it. Recall that our training data consists of the text and the corresponding label. We use a Naive Bayes classifier because it is the simplest classifier to begin with. It works in the following fashion: let's say we feed our algorithm `[\"I hate junk food\", \"neg\"]` it will assign probabilities for words \"I\", \"hate\", \"junk\", \"food\" to express a negative feeling. Once the training is complete, the algorithm will be able to generalize to reviews it hasn't seen yet by looking at the words in each review, and compute the probability for the sentence to be positive or negative.\n",
    "\n",
    "We start easy by only loading 100 reviews and the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "reviews = [c[0] for c in corpus.head(100, 'text_sentiment')]\n",
    "labels = [c[1] for c in corpus.head(100, 'text_sentiment')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then initialize a `CountVectorizer` to build the term-document matrix `X_train_counts`. You see that it has one row per review, and one column per vocabulary word, as we explained earlier. So here, there are 2176 unique words within our 100 first reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term-document matrix shape is: (100, 2176)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "X_train_counts = count_vectorizer.fit_transform(reviews)\n",
    "\n",
    "print \"term-document matrix shape is:\", X_train_counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize our classifier, then train it with term-document matrix and target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.61 ms, sys: 2.37 ms, total: 4.98 ms\n",
      "Wall time: 2.73 ms\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "multinomial_naive_bayes = MultinomialNB()\n",
    "%time clf = multinomial_naive_bayes.fit(X_train_counts, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, now let's try it by making up reviews and see if it can guess the sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"this is good food\" classified as: ['pos']\n",
      "\"that place was bad\" classified as: ['neg']\n"
     ]
    }
   ],
   "source": [
    "print '\"this is good food\" classified as:',\\\n",
    "    clf.predict(count_vectorizer.transform(['this is good food']))\n",
    "print '\"that place was bad\" classified as:',\\\n",
    "    clf.predict(count_vectorizer.transform(['that place was bad']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hoorray it worked ! But wait, this is actually just the beginning, we have no idea whether our model really works, or if we just got lucky with two fairly easy examples. Read the following to discover how we can improve this classifier !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first try, let's keep the same model we used, and train it on more data. Also, we will evaluate more precisely how our model performed, and to do so we are going to split the dataset in training and test data. Usually, we use 80% and 20% size for training and test respectively. Our goal is to see how well the model we trained is capable of generalizing on reviews it hasn't seen in training data.\n",
    "\n",
    "Let's load 20000 reviews and their labels, then split this dataset in training and test sets thanks to `scikit-learn` `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = [c[0] for c in corpus.head(20000, 'text_sentiment')],\\\n",
    "        [c[1] for c in corpus.head(20000, 'text_sentiment')]\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will train the model. We now use a `Pipeline`, which is a Class allowing to easily chain steps, like our vectorizer followed by the classifier. Once trained, `metrics` module displays some useful training results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.41 s, sys: 166 ms, total: 2.58 s\n",
      "Wall time: 2.51 s\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.77      0.72      0.75      1433\n",
      "        pos       0.85      0.88      0.87      2567\n",
      "\n",
      "avg / total       0.82      0.82      0.82      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf_1 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "%time text_clf_1.fit(X_train, y_train)\n",
    "\n",
    "print metrics.classification_report(y_test, text_clf_1.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.82 precision score, which means about 80% of our reviews have been correctly classified. Is it good ? Not really... we sure can do better !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying another model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.54 s, sys: 84.4 ms, total: 2.62 s\n",
      "Wall time: 2.65 s\n",
      "\n",
      "text_clf_2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.90      0.61      0.73      1433\n",
      "        pos       0.82      0.96      0.88      2567\n",
      "\n",
      "avg / total       0.85      0.84      0.83      4000\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "text_clf_2 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('clf', SGDClassifier(loss='hinge', \n",
    "                          penalty='l2', \n",
    "                          alpha=1e-3, \n",
    "                          n_iter=5, \n",
    "                          random_state=42)),\n",
    "    ])\n",
    "\n",
    "%time text_clf_2.fit(X_train, y_train)\n",
    "print '\\ntext_clf_2\\n %s \\n' %metrics.classification_report(y_test, \n",
    "                                                            text_clf_2.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pos']\n",
      "['neg']\n"
     ]
    }
   ],
   "source": [
    "print text_clf_2.predict(['oh I love this place it is so good the food is nice'])\n",
    "print text_clf_2.predict(['the food was really bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2), ((1, 3))],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'clf__alpha': (1e-2, 1e-3) \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gs_clf = GridSearchCV(text_clf_2, parameters, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.6 s, sys: 1.38 s, total: 13 s\n",
      "Wall time: 3min 28s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        st...     penalty='l2', power_t=0.5, random_state=42, shuffle=True, verbose=0,\n",
       "       warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'vect__ngram_range': [(1, 1), (1, 2), (1, 3)], 'tfidf__use_idf': (True, False), 'clf__alpha': (0.01, 0.001)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>param_clf__alpha</th>\n",
       "      <th>param_tfidf__use_idf</th>\n",
       "      <th>param_vect__ngram_range</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.464347</td>\n",
       "      <td>1.183293</td>\n",
       "      <td>0.815149</td>\n",
       "      <td>0.840485</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 1), u'tfidf__use_id...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.807520</td>\n",
       "      <td>0.841245</td>\n",
       "      <td>0.814375</td>\n",
       "      <td>0.842064</td>\n",
       "      <td>0.823556</td>\n",
       "      <td>0.838146</td>\n",
       "      <td>0.069137</td>\n",
       "      <td>0.020419</td>\n",
       "      <td>0.006570</td>\n",
       "      <td>0.001687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.527857</td>\n",
       "      <td>1.168161</td>\n",
       "      <td>0.793806</td>\n",
       "      <td>0.807724</td>\n",
       "      <td>0.001</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 1), u'tfidf__use_id...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.790734</td>\n",
       "      <td>0.806538</td>\n",
       "      <td>0.788401</td>\n",
       "      <td>0.809828</td>\n",
       "      <td>0.802284</td>\n",
       "      <td>0.806805</td>\n",
       "      <td>0.112910</td>\n",
       "      <td>0.036887</td>\n",
       "      <td>0.006069</td>\n",
       "      <td>0.001492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10.088305</td>\n",
       "      <td>3.299886</td>\n",
       "      <td>0.790896</td>\n",
       "      <td>0.810709</td>\n",
       "      <td>0.001</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 2), u'tfidf__use_id...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.789167</td>\n",
       "      <td>0.810233</td>\n",
       "      <td>0.784147</td>\n",
       "      <td>0.808708</td>\n",
       "      <td>0.799373</td>\n",
       "      <td>0.813186</td>\n",
       "      <td>0.127348</td>\n",
       "      <td>0.033627</td>\n",
       "      <td>0.006335</td>\n",
       "      <td>0.001859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>21.759013</td>\n",
       "      <td>3.852263</td>\n",
       "      <td>0.779925</td>\n",
       "      <td>0.799515</td>\n",
       "      <td>0.001</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 3), u'tfidf__use_id...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.779096</td>\n",
       "      <td>0.802060</td>\n",
       "      <td>0.772056</td>\n",
       "      <td>0.797515</td>\n",
       "      <td>0.788625</td>\n",
       "      <td>0.798970</td>\n",
       "      <td>0.180668</td>\n",
       "      <td>0.072844</td>\n",
       "      <td>0.006789</td>\n",
       "      <td>0.001895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11.589886</td>\n",
       "      <td>3.412248</td>\n",
       "      <td>0.722239</td>\n",
       "      <td>0.725859</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 2), u'tfidf__use_id...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.727171</td>\n",
       "      <td>0.731079</td>\n",
       "      <td>0.721451</td>\n",
       "      <td>0.723416</td>\n",
       "      <td>0.718092</td>\n",
       "      <td>0.723080</td>\n",
       "      <td>0.499936</td>\n",
       "      <td>0.095240</td>\n",
       "      <td>0.003748</td>\n",
       "      <td>0.003694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24.081614</td>\n",
       "      <td>5.390060</td>\n",
       "      <td>0.646791</td>\n",
       "      <td>0.637500</td>\n",
       "      <td>0.001</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 3), u'tfidf__use_id...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.646822</td>\n",
       "      <td>0.639275</td>\n",
       "      <td>0.644872</td>\n",
       "      <td>0.637788</td>\n",
       "      <td>0.648679</td>\n",
       "      <td>0.635438</td>\n",
       "      <td>0.165116</td>\n",
       "      <td>0.091102</td>\n",
       "      <td>0.001554</td>\n",
       "      <td>0.001580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.691615</td>\n",
       "      <td>1.147724</td>\n",
       "      <td>0.628731</td>\n",
       "      <td>0.628657</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 1), u'tfidf__use_id...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.628469</td>\n",
       "      <td>0.628751</td>\n",
       "      <td>0.629198</td>\n",
       "      <td>0.628498</td>\n",
       "      <td>0.628527</td>\n",
       "      <td>0.628722</td>\n",
       "      <td>0.340326</td>\n",
       "      <td>0.046332</td>\n",
       "      <td>0.000331</td>\n",
       "      <td>0.000113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.713394</td>\n",
       "      <td>1.354113</td>\n",
       "      <td>0.628507</td>\n",
       "      <td>0.628507</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 1), u'tfidf__use_id...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.628469</td>\n",
       "      <td>0.628527</td>\n",
       "      <td>0.628527</td>\n",
       "      <td>0.628498</td>\n",
       "      <td>0.628527</td>\n",
       "      <td>0.628498</td>\n",
       "      <td>0.080793</td>\n",
       "      <td>0.047543</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.796500</td>\n",
       "      <td>4.137727</td>\n",
       "      <td>0.628507</td>\n",
       "      <td>0.628507</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 2), u'tfidf__use_id...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.628469</td>\n",
       "      <td>0.628527</td>\n",
       "      <td>0.628527</td>\n",
       "      <td>0.628498</td>\n",
       "      <td>0.628527</td>\n",
       "      <td>0.628498</td>\n",
       "      <td>1.050651</td>\n",
       "      <td>0.703418</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24.997063</td>\n",
       "      <td>5.352116</td>\n",
       "      <td>0.628507</td>\n",
       "      <td>0.628507</td>\n",
       "      <td>0.01</td>\n",
       "      <td>True</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 3), u'tfidf__use_id...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.628469</td>\n",
       "      <td>0.628527</td>\n",
       "      <td>0.628527</td>\n",
       "      <td>0.628498</td>\n",
       "      <td>0.628527</td>\n",
       "      <td>0.628498</td>\n",
       "      <td>1.419840</td>\n",
       "      <td>0.046664</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.987088</td>\n",
       "      <td>3.243905</td>\n",
       "      <td>0.628507</td>\n",
       "      <td>0.628507</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 2), u'tfidf__use_id...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.628469</td>\n",
       "      <td>0.628527</td>\n",
       "      <td>0.628527</td>\n",
       "      <td>0.628498</td>\n",
       "      <td>0.628527</td>\n",
       "      <td>0.628498</td>\n",
       "      <td>0.243547</td>\n",
       "      <td>0.083843</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>22.851296</td>\n",
       "      <td>5.044871</td>\n",
       "      <td>0.628507</td>\n",
       "      <td>0.628507</td>\n",
       "      <td>0.01</td>\n",
       "      <td>False</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>{u'vect__ngram_range': (1, 3), u'tfidf__use_id...</td>\n",
       "      <td>8</td>\n",
       "      <td>0.628469</td>\n",
       "      <td>0.628527</td>\n",
       "      <td>0.628527</td>\n",
       "      <td>0.628498</td>\n",
       "      <td>0.628527</td>\n",
       "      <td>0.628498</td>\n",
       "      <td>0.207925</td>\n",
       "      <td>0.053228</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  mean_score_time  mean_test_score  mean_train_score  \\\n",
       "6        2.464347         1.183293         0.815149          0.840485   \n",
       "9        2.527857         1.168161         0.793806          0.807724   \n",
       "10      10.088305         3.299886         0.790896          0.810709   \n",
       "11      21.759013         3.852263         0.779925          0.799515   \n",
       "7       11.589886         3.412248         0.722239          0.725859   \n",
       "8       24.081614         5.390060         0.646791          0.637500   \n",
       "3        2.691615         1.147724         0.628731          0.628657   \n",
       "0        2.713394         1.354113         0.628507          0.628507   \n",
       "1       13.796500         4.137727         0.628507          0.628507   \n",
       "2       24.997063         5.352116         0.628507          0.628507   \n",
       "4        9.987088         3.243905         0.628507          0.628507   \n",
       "5       22.851296         5.044871         0.628507          0.628507   \n",
       "\n",
       "   param_clf__alpha param_tfidf__use_idf param_vect__ngram_range  \\\n",
       "6             0.001                 True                  (1, 1)   \n",
       "9             0.001                False                  (1, 1)   \n",
       "10            0.001                False                  (1, 2)   \n",
       "11            0.001                False                  (1, 3)   \n",
       "7             0.001                 True                  (1, 2)   \n",
       "8             0.001                 True                  (1, 3)   \n",
       "3              0.01                False                  (1, 1)   \n",
       "0              0.01                 True                  (1, 1)   \n",
       "1              0.01                 True                  (1, 2)   \n",
       "2              0.01                 True                  (1, 3)   \n",
       "4              0.01                False                  (1, 2)   \n",
       "5              0.01                False                  (1, 3)   \n",
       "\n",
       "                                               params  rank_test_score  \\\n",
       "6   {u'vect__ngram_range': (1, 1), u'tfidf__use_id...                1   \n",
       "9   {u'vect__ngram_range': (1, 1), u'tfidf__use_id...                2   \n",
       "10  {u'vect__ngram_range': (1, 2), u'tfidf__use_id...                3   \n",
       "11  {u'vect__ngram_range': (1, 3), u'tfidf__use_id...                4   \n",
       "7   {u'vect__ngram_range': (1, 2), u'tfidf__use_id...                5   \n",
       "8   {u'vect__ngram_range': (1, 3), u'tfidf__use_id...                6   \n",
       "3   {u'vect__ngram_range': (1, 1), u'tfidf__use_id...                7   \n",
       "0   {u'vect__ngram_range': (1, 1), u'tfidf__use_id...                8   \n",
       "1   {u'vect__ngram_range': (1, 2), u'tfidf__use_id...                8   \n",
       "2   {u'vect__ngram_range': (1, 3), u'tfidf__use_id...                8   \n",
       "4   {u'vect__ngram_range': (1, 2), u'tfidf__use_id...                8   \n",
       "5   {u'vect__ngram_range': (1, 3), u'tfidf__use_id...                8   \n",
       "\n",
       "    split0_test_score  split0_train_score  split1_test_score  \\\n",
       "6            0.807520            0.841245           0.814375   \n",
       "9            0.790734            0.806538           0.788401   \n",
       "10           0.789167            0.810233           0.784147   \n",
       "11           0.779096            0.802060           0.772056   \n",
       "7            0.727171            0.731079           0.721451   \n",
       "8            0.646822            0.639275           0.644872   \n",
       "3            0.628469            0.628751           0.629198   \n",
       "0            0.628469            0.628527           0.628527   \n",
       "1            0.628469            0.628527           0.628527   \n",
       "2            0.628469            0.628527           0.628527   \n",
       "4            0.628469            0.628527           0.628527   \n",
       "5            0.628469            0.628527           0.628527   \n",
       "\n",
       "    split1_train_score  split2_test_score  split2_train_score  std_fit_time  \\\n",
       "6             0.842064           0.823556            0.838146      0.069137   \n",
       "9             0.809828           0.802284            0.806805      0.112910   \n",
       "10            0.808708           0.799373            0.813186      0.127348   \n",
       "11            0.797515           0.788625            0.798970      0.180668   \n",
       "7             0.723416           0.718092            0.723080      0.499936   \n",
       "8             0.637788           0.648679            0.635438      0.165116   \n",
       "3             0.628498           0.628527            0.628722      0.340326   \n",
       "0             0.628498           0.628527            0.628498      0.080793   \n",
       "1             0.628498           0.628527            0.628498      1.050651   \n",
       "2             0.628498           0.628527            0.628498      1.419840   \n",
       "4             0.628498           0.628527            0.628498      0.243547   \n",
       "5             0.628498           0.628527            0.628498      0.207925   \n",
       "\n",
       "    std_score_time  std_test_score  std_train_score  \n",
       "6         0.020419        0.006570         0.001687  \n",
       "9         0.036887        0.006069         0.001492  \n",
       "10        0.033627        0.006335         0.001859  \n",
       "11        0.072844        0.006789         0.001895  \n",
       "7         0.095240        0.003748         0.003694  \n",
       "8         0.091102        0.001554         0.001580  \n",
       "3         0.046332        0.000331         0.000113  \n",
       "0         0.047543        0.000027         0.000014  \n",
       "1         0.703418        0.000027         0.000014  \n",
       "2         0.046664        0.000027         0.000014  \n",
       "4         0.083843        0.000027         0.000014  \n",
       "5         0.053228        0.000027         0.000014  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gs_clf.cv_results_ ).sort_values(by='mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
